import os   #biblioteca para trabalhar com arquivos
import csv  #biblioteca para trabalhar com arquivos csv
import json #biblioteca para trabalhar com arquivos json
import logging #biblioteca para trabalhar com logs
from etl.utils import (
                        validate_csv_exists, 
                        validate_data, 
                        validate_schema, 
                        validate_required_fields,
                        validate_batch_records
                 ) #funcoes auxiliares para validar os arquivos, os dados e o schema de um dicionario 

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger(__name__)

class ETL: # classe para realizar a extra√ß√£o, transforma√ß√£o  de CSV -> JSON 
    def __init__(self, input_path, output_path):  # construtor da classe
        self.input_path = input_path              # atributos da classe
        self.output_path = output_path            # atributos da classe

        # Schema esperado - movido para o construtor para melhor organiza√ß√£o
        self.schema = {
            "TITLE": str,
            "F_NAME": str,
            "L_NAME": str,
            "GENDER": str,
            "MONTH_AND_DATE": str,
            "DOB": str,
            "YOB": int,
            "EMAIL": str,
            "ID1": str,
            "ID2": str,
            "ID3": str,
            "ID4": str,
            "PHONE": str,
            "EMAIL2": str,
            "STREET": str,
            "CITY": str,
            "STATE": str,
            "COUNTRY": str,
            "ZIP": str,
            "LAT": float,
            "LONG": float,
        }
                # Campos obrigat√≥rios (voc√™ pode personalizar conforme sua necessidade)
        self.required_fields = [
            "F_NAME", 
            "L_NAME", 
            "EMAIL", 
            "PHONE"
        ]



    def extract(self):   # funcao para extrair os dados
        logging.info(f"üìÇ Extraindo dados de {self.input_path}") #mostra  de onde est√° extraindo os dados 
        validate_csv_exists(self.input_path) #valida se o arquivo CSV existe

        try:
            with open(self.input_path, mode='r', encoding='utf-8') as file: #abre o arquivo
                reader = csv.DictReader(file) #le o arquivo CSV e transforma em um dicionario
                data = list(reader) #transforma o arquivo em uma lista de dicion√°rios

            logging.info(f"‚úÖ {len(data)} registros extra√≠dos.")  #mostra quantos registros foram encontrados
            return data
        
        except Exception as e:
            logging.error(f"‚ùå Erro ao extrair dados: {e}")
            raise 

    def transform(self, data):  # Fun√ß√£o para transformar e validar os dados
        logger.info("üîß Transformando dados...")

        # Valida√ß√£o inicial dos dados
        if not validate_data(data):
            raise ValueError("Nenhum dado extra√≠do ou dados inv√°lidos.")

        # NOVA IMPLEMENTA√á√ÉO: Valida√ß√£o pr√©via em lote
        logger.info("üîç Executando valida√ß√£o pr√©via em lote...")
        
        # Primeiro, limpa os dados (remove espa√ßos)
        cleaned_data = []
        for row in data:
            cleaned_row = {k.strip(): v.strip() if isinstance(v, str) else v for k, v in row.items()}
            cleaned_data.append(cleaned_row)

        # Valida√ß√£o em lote dos dados limpos
        validation_report = validate_batch_records(cleaned_data, self.schema)
        
        # Log do relat√≥rio de valida√ß√£o
        logger.info(f"üìä Relat√≥rio de Valida√ß√£o Inicial:")
        logger.info(f"   ‚Ä¢ Total de registros: {validation_report['total_records']}")
        logger.info(f"   ‚Ä¢ Registros v√°lidos: {validation_report['valid_records']}")
        logger.info(f"   ‚Ä¢ Registros inv√°lidos: {validation_report['invalid_records']}")
        logger.info(f"   ‚Ä¢ Taxa de sucesso: {validation_report['success_rate']:.1f}%")
        
        if validation_report['invalid_lines']:
            logger.warning(f"   ‚ö†Ô∏è Linhas com problemas: {validation_report['invalid_lines']}")

        # Processa cada registro individualmente
        transformed_data = []
        valid_count = 0
        invalid_count = 0

        for i, row in enumerate(cleaned_data):
            try:
                # Valida√ß√£o de campos obrigat√≥rios
                if not validate_required_fields(row, self.required_fields):
                    logger.warning(f"‚ö†Ô∏è Registro {i+1} ignorado: campos obrigat√≥rios ausentes")
                    invalid_count += 1
                    continue

                # Valida√ß√£o de schema (verifica√ß√£o adicional individual)
                if validate_schema(row, self.schema):
                    transformed_data.append(row)
                    valid_count += 1
                else:
                    logger.warning(f"‚ö†Ô∏è Registro {i+1} ignorado: schema inv√°lido")
                    invalid_count += 1
                    
            except Exception as e:
                logger.error(f"‚ùå Erro ao processar registro {i+1}: {e}")
                invalid_count += 1
                continue

        # Relat√≥rio final da transforma√ß√£o
        total_processed = valid_count + invalid_count
        final_success_rate = (valid_count / total_processed * 100) if total_processed > 0 else 0
        
        logger.info(f"‚úÖ Transforma√ß√£o conclu√≠da:")
        logger.info(f"   ‚Ä¢ Registros processados: {total_processed}")
        logger.info(f"   ‚Ä¢ Registros v√°lidos finais: {valid_count}")
        logger.info(f"   ‚Ä¢ Registros rejeitados: {invalid_count}")
        logger.info(f"   ‚Ä¢ Taxa de sucesso final: {final_success_rate:.1f}%")

        if not transformed_data:
            raise ValueError("Nenhum registro v√°lido ap√≥s transforma√ß√£o")

        return transformed_data

    def load(self, data):   #funcao para carregar os dados transformados em um arquivo JSON
            logger.info(f"üíæ Salvando dados em {self.output_path}")  # imprime onde estao salvando os dados
            
            try:
                os.makedirs(os.path.dirname(self.output_path), exist_ok=True) #cria o diret√≥rio onde o arquivo vai ser salvo

                with open(self.output_path, mode='w', encoding='utf-8') as file: #abre o arquivo
                    json.dump(data, file, indent=4, ensure_ascii=False) #salva o arquivo
                logging.info(f"‚úÖ Dados salvos em {self.output_path}")

            except Exception as e:
                logging.error(f"‚ùå Erro ao salvar dados: {e}")
                raise
        

    def run(self):  #funcao para executar o ETL
        try:
            data = self.extract()  #extrai os dados
            validate_data (data)    #valida os dados
            data = self.trasform(data)   #limapa os dados
            self.load(data) #carrega os dados em um arquivo JSON

        except Exception as e:
            logging.error(f"‚ùå Erro ao executar o ETL: {e}")
            raise

        logging.info("üéâ ETL concluido com sucesso!")
    def get_validation_summary(self, data): #funcao para obter um resumo da valida√ß√£o
        logger.info("üîç Obtendo resumo de valida√ß√£o...")
        if not data:
            return {"error": "Dados vazios"}
            
        # Limpa os dados primeiro
        cleaned_data = []
        for row in data:
            cleaned_row = {k.strip(): v.strip() if isinstance(v, str) else v for k, v in row.items()}
            cleaned_data.append(cleaned_row)
        
        # Valida√ß√£o em lote
        batch_report = validate_batch_records(cleaned_data, self.schema)
        
        # Valida√ß√£o de campos obrigat√≥rios
        missing_required_count = 0
        for row in cleaned_data:
            if not validate_required_fields(row, self.required_fields):
                missing_required_count += 1
        
        return {
            **batch_report,
            'missing_required_fields_count': missing_required_count,
            'required_fields': self.required_fields,
            'schema_fields': list(self.schema.keys())
        }


if __name__ == "__main__":  #verifica se o arquivo foi executado diretamente como um script principal   
    input_file_name = 'CRM_profiles.csv'
    input_file = os.path.join('data', 'input', input_file_name)
    output_file = os.path.join('data', 'output', input_file_name.replace('.csv', '.json'))


    etl = ETL(input_file, output_file)  #cria uma instancia da classe
    etl.run()   #executa o ETL



# ETL de CSV para JSON

![Python](https://img.shields.io/badge/Python-3.11%2B-blue?style=for-the-badge&logo=python)
![Tests](https://img.shields.io/badge/Tests-Pytest-green?style=for-the-badge&logo=pytest)
![Coverage](https://img.shields.io/badge/Coverage-95%25-brightgreen?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-purple?style=for-the-badge)

> Pipeline de ETL (Extract, Transform, Load) construÃ­do em Python. Este projeto processa arquivos CSV de perfis de clientes, aplica validaÃ§Ãµes rigorosas de schema e transforma os dados, salvando o resultado em formato JSON. Idealizado como uma peÃ§a de portfÃ³lio para demonstrar boas prÃ¡ticas de desenvolvimento, incluindo testes automatizados, logging detalhado e cÃ³digo modular.

<br>

##  navegar
* [ğŸ“Œ Sobre o Projeto](#-sobre-o-projeto)
* [âœ¨ Funcionalidades](#-funcionalidades)
* [âš™ï¸ O Processo de TransformaÃ§Ã£o em Detalhes](#ï¸-o-processo-de-transformaÃ§Ã£o-em-detalhes)
* [ğŸ’» Tecnologias Utilizadas](#-tecnologias-utilizadas)
* [ğŸš€ Como Executar o Projeto](#-como-executar-o-projeto)
* [ğŸ§ª Como Rodar os Testes](#-como-rodar-os-testes)
* [ğŸ“ Estrutura de Pastas](#-estrutura-de-pastas)
* [ğŸ§  PrÃ³ximos Passos](#-prÃ³ximos-passos)
* [ğŸ‘¨â€ğŸ’» Autor](#-autor)

---

## ğŸ“Œ Sobre o Projeto

O objetivo deste ETL Ã© garantir a integridade e a qualidade dos dados de perfis de clientes antes de serem consumidos por outros sistemas. O pipeline extrai dados de um arquivo `CRM_profiles.csv`, executa um processo de validaÃ§Ã£o e limpeza em duas etapas, e carrega os dados 100% conformes em um arquivo `CRM_profiles.json`. Erros e avisos sÃ£o registrados em logs para total rastreabilidade do processo.

---

## âœ¨ Funcionalidades

-   **ExtraÃ§Ã£o (Extract):** Leitura eficiente de dados de arquivos `.csv` utilizando `DictReader` para processar registros como dicionÃ¡rios.
-   **TransformaÃ§Ã£o (Transform):**
    -   **Limpeza de Dados:** Remove automaticamente espaÃ§os em branco desnecessÃ¡rios das chaves e valores de cada registro.
    -   **ValidaÃ§Ã£o em Lote:** Realiza uma validaÃ§Ã£o prÃ©via em todos os dados para gerar um relatÃ³rio rÃ¡pido sobre a saÃºde geral do arquivo, com a taxa de sucesso inicial.
    -   **ValidaÃ§Ã£o Individual Rigorosa:** Cada registro Ã© verificado para garantir a presenÃ§a de campos obrigatÃ³rios e a conformidade com o schema de tipos de dados.
    -   **TolerÃ¢ncia a Falhas:** Registros invÃ¡lidos sÃ£o descartados e logados como `warning` sem interromper o pipeline, garantindo que todos os dados vÃ¡lidos sejam processados.
-   **Carga (Load):**
    -   CriaÃ§Ã£o automÃ¡tica do diretÃ³rio de saÃ­da, se nÃ£o existir.
    -   Salvamento dos dados limpos e vÃ¡lidos em formato `.json` legÃ­vel e bem formatado.
-   **Logging Detalhado:** Registra cada etapa (`INFO`), avisos de registros invÃ¡lidos (`WARNING`) e erros crÃ­ticos (`ERROR`), com timestamps para fÃ¡cil depuraÃ§Ã£o.

---

## âš™ï¸ O Processo de TransformaÃ§Ã£o em Detalhes

A etapa de transformaÃ§Ã£o Ã© o coraÃ§Ã£o deste projeto e segue um fluxo robusto para garantir a mÃ¡xima qualidade dos dados:

1.  **Limpeza Inicial:** Antes de qualquer validaÃ§Ã£o, todos os registros passam por uma limpeza, onde espaÃ§os em branco no inÃ­cio e no fim das chaves e valores sÃ£o removidos.

2.  **ValidaÃ§Ã£o em Lote (PrÃ©via):** O sistema realiza uma primeira anÃ¡lise em todos os registros para gerar um relatÃ³rio de saÃºde dos dados. Isso oferece uma visÃ£o macro da qualidade do arquivo de entrada, com a porcentagem de registros conformes.

3.  **ValidaÃ§Ã£o Individual:** Cada registro Ã© entÃ£o validado individualmente contra dois critÃ©rios principais:
    -   **PresenÃ§a de Campos ObrigatÃ³rios:** Verifica se os seguintes campos existem e nÃ£o estÃ£o vazios: `F_NAME`, `L_NAME`, `EMAIL`, `PHONE`.
    -   **Conformidade com o Schema:** Garante que cada campo corresponde ao tipo de dado esperado. Por exemplo, `YOB` deve ser um inteiro (`int`) e `LAT` um nÃºmero de ponto flutuante (`float`).

4.  **Tratamento de Registros:** Registros que falham em qualquer uma das validaÃ§Ãµes individuais sÃ£o descartados, e uma entrada de log (`WARNING`) Ã© gerada, especificando o motivo da falha. O processo continua, assegurando que apenas os dados 100% vÃ¡lidos cheguem ao arquivo final.

<details>
<summary>ğŸ“– Clique para ver o Schema de ValidaÃ§Ã£o Completo</summary>

```python
{
    "TITLE": str, "F_NAME": str, "L_NAME": str, "GENDER": str,
    "MONTH_AND_DATE": str, "DOB": str, "YOB": int, "EMAIL": str,
    "ID1": str, "ID2": str, "ID3": str, "ID4": str, "PHONE": str,
    "EMAIL2": str, "STREET": str, "CITY": str, "STATE": str,
    "COUNTRY": str, "ZIP": str, "LAT": float, "LONG": float,
}
```
</details>

---

## ğŸ’» Tecnologias Utilizadas

Este projeto foi construÃ­do com as seguintes tecnologias:

-   **Python 3.11+**
-   **Pytest** (para testes unitÃ¡rios)
-   **pytest-cov** (para relatÃ³rio de cobertura de testes)
-   MÃ³dulo `typing` para tipagem estÃ¡tica
-   MÃ³dulo `logging` para logs estruturados
-   MÃ³dulo `csv` e `json` para manipulaÃ§Ã£o de arquivos

---

## ğŸš€ Como Executar o Projeto

Siga os passos abaixo para executar o projeto localmente.

### **PrÃ©-requisitos**

-   [Python 3.11](https://www.python.org/downloads/) ou superior
-   [Git](https://git-scm.com/downloads)

### **Passo a Passo**

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/seu-usuario/ETL_CSV_TO_JSON.git](https://github.com/seu-usuario/ETL_CSV_TO_JSON.git)
    cd ETL_CSV_TO_JSON
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    # Linux / macOS
    python3 -m venv venv
    source venv/bin/activate

    # Windows
    python -m venv venv
    .\venv\Scripts\activate
    ```

3.  **Instale as dependÃªncias:**
    *(ObservaÃ§Ã£o: Crie um arquivo `requirements.txt` com as bibliotecas `pytest` e `pytest-cov`)*
    ```bash
    pip install -r requirements.txt
    ```

4.  **Execute o Pipeline ETL:**
    O script principal irÃ¡ ler o arquivo de `data/input/CRM_profiles.csv` e gerar a saÃ­da em `data/output/CRM_profiles.json`.
    ```bash
    python etl/main.py
    ```

---

## ğŸ§ª Como Rodar os Testes

Os testes unitÃ¡rios foram criados para validar cada componente do pipeline.

Para executar os testes e ver o relatÃ³rio de cobertura no terminal, rode o comando na raiz do projeto:

```bash
pytest --cov=etl --cov-report term-missing
```

---

## ğŸ“ Estrutura de Pastas

O projeto estÃ¡ organizado da seguinte forma para garantir clareza e manutenibilidade:

```
ETL_CSV_TO_JSON/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â””â”€â”€ CRM_profiles.csv  # Arquivo de dados brutos de entrada
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ CRM_profiles.json # Arquivo de saÃ­da com dados processados
â”‚
â”œâ”€â”€ etl/                # MÃ³dulo principal da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py         # Ponto de entrada (entrypoint) para executar o pipeline
â”‚   â”œâ”€â”€ pipeline.py     # ContÃ©m a classe e a lÃ³gica do ETL
â”‚   â””â”€â”€ utils.py        # FunÃ§Ãµes auxiliares de validaÃ§Ã£o
â”‚
â”œâ”€â”€ tests/              # SuÃ­te de testes automatizados
â”‚   â””â”€â”€ test_etl.py     # Testes unitÃ¡rios para o pipeline
â”‚
â”œâ”€â”€ .gitignore          # Arquivos e pastas a serem ignorados pelo Git
â””â”€â”€ readme.md           # DocumentaÃ§Ã£o do projeto
```

---

## ğŸ§  PrÃ³ximos Passos

-   [ ] IntegraÃ§Ã£o com um banco de dados (ex: PostgreSQL, SQLite) como destino (Load).
-   [ ] Upload automÃ¡tico do arquivo JSON gerado para um serviÃ§o de nuvem (ex: AWS S3).
-   [ ] CriaÃ§Ã£o de uma Interface de Linha de Comando (CLI) com `argparse` para passar argumentos (ex: caminho do arquivo de entrada/saÃ­da).
-   [ ] OrquestraÃ§Ã£o do pipeline com ferramentas como Airflow ou Prefect.

---

## ğŸ‘¨â€ğŸ’» Autor

**Felipe GalvÃ£o**

-   LinkedIn: [`linkedin.com/in/felipe-galvao-data`](https://www.linkedin.com/in/felipe-galvao-data/)
-   GitHub: [`github.com/felipegalvao`](https://github.com/felipegalvao)